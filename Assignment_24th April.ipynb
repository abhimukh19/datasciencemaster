{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935faf4c-e61e-4fb2-96c5-5e0091d2ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511dce4-28b7-4a8a-8ad5-ed29ab056f36",
   "metadata": {},
   "source": [
    "It is a statistical technique used to reduce the dimensionality of a dataset while retaining as much of its variation as possible. \n",
    "\n",
    "PCA projects high-dimensional data onto a lower-dimensional space while preserving as much of the variation in the data as possible. \n",
    "\n",
    "In PCA, a projection matrix is created that maps each data point onto a lower-dimensional space. The projection matrix is created by finding the eigenvectors and eigenvalues of the covariance matrix of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd49d81a-c814-47f7-880f-a7c941f3bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e7b4a-d815-4f6f-babe-b96dfafe3f51",
   "metadata": {},
   "source": [
    "PCA can be formulated as an optimization problem that seeks to minimize the reconstruction error of the original data after it has been projected onto a lower-dimensional space\n",
    "\n",
    "The optimization problem in PCA is solved by finding the eigenvectors and eigenvalues of the covariance matrix of the original data. The eigenvectors are then used to create a projection matrix that maps each data point onto a lower-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed151d47-1617-4a44-8e11-bbb59bd8a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6ef2b-ee36-4322-8766-c51cd1b2c2e0",
   "metadata": {},
   "source": [
    "The PCA is to perform the eigendecomposition on the covariance matrix, which is a e Ã— e matrix where each element represents the covariance between two features.The eigenvalues explain the variance of the data along the new feature axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85eb511-8822-4312-9f52-83d76b6e2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ee7cb-4227-4fd4-9b20-eb0237205d5d",
   "metadata": {},
   "source": [
    "The choice of the number of principal components to retain in PCA can impact the performance of PCA. \n",
    "\n",
    "If too few principal components are retained, then the resulting lower-dimensional representation may not capture enough of the variance in the original data. On the other hand, if too many principal components are retained, then the resulting lower-dimensional representation may capture noise in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c46f47a-d9a3-48c1-be8d-ccde84e96b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44092487-363e-4e2a-bb5c-6a21b8b00732",
   "metadata": {},
   "source": [
    "PCA can be used for feature selection by selecting the top k principal components that capture the most variance in the data. The selected principal components can then be used as the new features for a machine learning algorithm.\n",
    "\n",
    "One benefit of using PCA for feature selection is that it can reduce the dimensionality of the data, which can help to reduce overfitting and improve the performance of the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36acd0e2-7b59-46c9-b42d-2013055fbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d2aaf-a833-4d26-9d6d-8903a23b65cd",
   "metadata": {},
   "source": [
    "Dimensionality reduction: PCA can be used to reduce the dimensionality of high-dimensional data while retaining most of the information in the data.\n",
    "\n",
    "Feature extraction: PCA can be used to extract features from high-dimensional data that are more informative than the original features.\n",
    "\n",
    "Data visualization: PCA can be used to visualize high-dimensional data in two or three dimensions.\n",
    "\n",
    "Noise reduction: PCA can be used to remove noise from data.\n",
    "\n",
    "Compression: PCA can be used to compress data while retaining most of the information in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb9eea0-dac7-498f-ae79-83427d00f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2ec97-f849-470b-b71b-bfc1f50de937",
   "metadata": {},
   "source": [
    "In PCA, the variance of a principal component represents the amount of information that is captured by that principal component. The spread of the data along a principal component is proportional to the square root of the variance of that principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730b01bb-ba6f-424c-9829-14a79a6a4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc128c-e72a-428a-8584-6db6394c1f1c",
   "metadata": {},
   "source": [
    "The spread and variance of the data are used to identify principal components because they provide information about how much information is captured by each principal component. The larger the variance of a principal component, the more information is captured by that principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef972510-b330-46d5-a0de-ac0d556606c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ad8be-0aae-49a3-abd8-4196f3a282dd",
   "metadata": {},
   "source": [
    "Suppose that we have a dataset with two features: feature 1 has high variance and feature 2 has low variance. In this case, the first principal component will be aligned with feature 1 because it captures the most variance in the data. The second principal component will be aligned with feature 2 because it captures the most variance among all directions that are orthogonal to the first principal component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32142af6-6fcd-430c-b152-01114607b9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
