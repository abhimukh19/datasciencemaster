{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497a221d-9387-4ec1-b993-74bb23b232a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff189d0-abb7-4961-99a1-bc44de434f4a",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique used over regression methods for a more accurate prediction. This model uses shrinkage where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models\n",
    "\n",
    "Lasso regression is an adaptation of linear regression algorithm. It enhances regular linear regression by slightly changing its cost function, which results in less overfit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5420e81-7f93-4196-af21-26d489cfb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f2673-a5a0-47a5-b595-cb754951c36a",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is that it has the ability to set the coefficients for features it does not consider relevant to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf09684-6318-402c-98df-1ebd08b14b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9425ff-b5f7-4cda-a646-ae2c5fd89601",
   "metadata": {},
   "source": [
    "In Lasso Regression, the coefficients are shrunk towards zero by adding a penalty term to the loss function of the linear regression model. \n",
    "\n",
    "The coefficients that are shrunk to zero are considered as irrelevant and removed from the model. The remaining coefficients are considered as important predictors\n",
    "\n",
    "One way to interpret the coefficients is by looking at their magnitude. The larger the magnitude of a coefficient, the more important its corresponding predictor variable is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba63efb-6c64-4701-91a8-36f82cd04f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "#model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e323ef-2aa9-4655-9ea5-5648777db1da",
   "metadata": {},
   "source": [
    "Lasso regression has a tuning parameter (λ), sometimes called a penalty parameter, that controls the strength of the L1 penalty. λ is basically the amount of shrinkage: When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression. As λ increases, more and more coefficients are set to zero and eliminated (theoretically, when λ = ∞, all coefficients are eliminated)\n",
    "\n",
    "The tuning parameter controls the trade-off between fitting the model well to the training data and keeping it simple enough to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad038f-7631-41ef-b8e9-db927811cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052ee75-bc14-459a-a83b-0a583f581a9a",
   "metadata": {},
   "source": [
    "Yes, Lasso regression can be used for non-linear regression problems.\n",
    "\n",
    "One way to do this is by using Gaussian basis functions with Lasso regularization. \n",
    "\n",
    "Another way is to make nonlinear terms through transformation (e.g. Lag1^2) and see if they stay in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dff29f-3f54-46fc-b9de-5e6a545807cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a60d8-903e-4031-999e-3d0e9e740cb9",
   "metadata": {},
   "source": [
    "Both Ridge Regression and Lasso Regression are regularization techniques used to prevent overfitting in linear regression models. The difference between them lies in the way they apply the penalty factor to the coefficients.\n",
    "\n",
    "Ridge Regression is used when all features are important and we want to reduce their impact on the model output, while Lasso Regression is used when we want to select a subset of important features and ignore others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10bdb4-f278-41e8-a0cc-bf2ffcdc0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f2cea-367c-4805-b811-14359d4e11d5",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in input features. Lasso regression is a linear regression technique with L1 prior as a regularizer. The idea is to reduce the multicollinearity by regularization by reducing the coefficients of the feature that are multicollinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043480cb-70b9-4842-be4f-d85baa5edbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb53997-4b95-4164-aa7e-37134d9c925b",
   "metadata": {},
   "source": [
    "To choose the optimal value of lambda in Lasso Regression, we can use cross-validation. We can fit several models using different values for lambda and choose lambda to be the value that produces the lowest test MSE. \n",
    "\n",
    "Another method is to use stability selection for LASSO with weights based on AUC. \n",
    "\n",
    "We can also use k-fold cross-validation to select lambda min and lambda 1se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a24b3d-c21b-4744-8abb-d81e10bc3382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
