{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68be2f46-3b77-4a1b-82a2-1d0664037610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24390d5b-741b-403a-ab8f-f5fce5277961",
   "metadata": {},
   "source": [
    "It is a non-parametric, supervised learning classifier that uses proximity to make classifications or predictions about the grouping of an individual data point\n",
    "\n",
    "Some of these use cases include data preprocessing where datasets frequently have missing values but the KNN algorithm can estimate for those values in a process known as missing data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22677013-df70-4b2b-b8e7-6291a51c5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a089065-1850-499c-9ddb-f72059e02c71",
   "metadata": {},
   "source": [
    "A simple approach to select K is to set K = n^(1/2) where n = number of data points in training data. \n",
    "\n",
    "Another approach is to choose an odd number if the number of classes is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca578d75-e039-4934-8afd-40f102ffe79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe07fc7-0898-44d7-a870-cdcb0de4f176",
   "metadata": {},
   "source": [
    "The key differences between KNN regression and KNN classifier are that KNN regression tries to predict the value of the output variable by using a local average while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802d7339-965a-4841-950f-da4a65fd8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86558082-9cf2-4c45-bce6-6aff35b27800",
   "metadata": {},
   "source": [
    "The performance of KNN can be measured using various metrics such as accuracy, precision, recall and F1-score\n",
    "\n",
    "One of the most commonly used parameter is the average accuracy which is defined by the following equation:\n",
    "\n",
    "Average Accuracy = (TPi + TNi) / (TPi + FNi + FPi + TNi)\n",
    "\n",
    "where TPi is true positive, TNi is true negative, FPi is false positive and FNi is false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65fcc56f-820c-402d-9072-10468dc446a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdccf2b-8e30-4354-aac8-100ddabe3f5c",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a phenomenon that occurs when the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset. KNN is very susceptible to overfitting due to the curse of dimensionality.\n",
    "\n",
    "When the number of features increases, then it requires more data. When thereâ€™s more data, it creates an overfitting problem because no one knows which piece of noise will contribute to the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d6d0a-c294-483f-b678-f548e3fd012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a9753-1854-452e-8221-5b8eec61f6e1",
   "metadata": {},
   "source": [
    "The assumption behind using KNN for missing values is that a point value can be approximated by the values of the points that are closest to it, based on other variables1. Datasets frequently have missing values, but the KNN algorithm can estimate for those values in a process known as missing data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ad4a93-20b6-4880-ab5e-b1846305b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d198c-1151-4db0-a2f4-2e8d1f343b92",
   "metadata": {},
   "source": [
    "The KNN classifier predicts a class by using the highest majority category among its k nearest neighbors while the KNN regressor predicts a value by using the mean of the k nearest neighbors\n",
    "\n",
    "The KNN classifier is better suited for classification problems while the KNN regressor is better suited for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e865c9-4927-4057-9260-994aad292c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "#and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4ccc8-0bfa-4552-bc16-fa831b92c8f6",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "Nonparametric architecture\n",
    "\n",
    "Simple and powerful\n",
    "\n",
    "Requires no training time\n",
    "\n",
    "Easy to implement\n",
    "\n",
    "Effective for classification as well as regression\n",
    "    \n",
    "Weaknesses:\n",
    "\n",
    "Memory intensive\n",
    "\n",
    "Classification and estimation are slow\n",
    "\n",
    "Sensitive to outliers\n",
    "\n",
    "To address these weaknesses, one can use techniques such as k-d trees which can reduce the memory requirements of the algorithm and distance weighting which can reduce the impact of outliers on the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a662cb1-4add-4ec5-bb50-eacd4c61db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd7105-61e4-493c-a87f-6b8eed729cae",
   "metadata": {},
   "source": [
    "The Euclidean distance and Manhattan distance are both distance metrics used in KNN algorithm. \n",
    "\n",
    "The Euclidean distance is calculated as the square root of the sum of the squared differences between two points. \n",
    "\n",
    "It gives the shortest or minimum distance between two points4. On the other hand, Manhattan distance calculates distance horizontally or vertically only4. It has dimension restrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c6966-9b1a-4098-a637-284fda8eab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534ca6b-5a84-4e7b-88d9-77e7ebee69c1",
   "metadata": {},
   "source": [
    "Feature scaling is required to get better performance of the KNN algorithm. This is because the distance calculation done in KNN uses feature values. When one feature values are large than others, that feature will dominate the distance hence the outcome of the KNN\n",
    "\n",
    "Feature scaling through standardization, also called Z-score normalization, involves rescaling each feature such that it has a standard deviation of 1 and a mean of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc910bec-c2e9-417a-95a2-36f08bffac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
